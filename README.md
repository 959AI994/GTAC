# GTAC: A Generative Transformer for Approximate Circuits

## Overview

Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce **GTAC**, a novel **generative Transformer-based** model for producing approximate circuits. By integrating error thresholds into the design process, GTAC enables approximation-aware synthesis. Experimental results show that, compared with a state-of-the-art method, GTAC further reduces **6.4% area** under the error-rate constraint, while being **4.3Ã— faster**.

This repository provides an implementation of GTAC for **approximate logic synthesis**. Given a logic circuit (AIG/AAG), the model generates an optimized/approximated circuit under a user-specified error-rate constraint, optionally enhanced with Monte-Carlo Tree Search (MCTS).

> **Note**: This is an **anonymized** repository for double-blind review. Identifying information and links will be restored upon acceptance.

## Repository Structure

- `gtac/`: core library (encoding, training, inference, MCTS, utilities)
  - `gtac/model/`: training + inference implementations
  - `gtac/bin/`: bundled EDA tools (`abc`, `aigtoaig`) used by the pipeline
- `scripts/utilities/run_example.py`: **single entry-point example** for training + inference (used in this README)
- `requirements.txt`: Python dependencies

## Datasets

All circuits are stored in **AIGER** formats (`.aig` / `.aag`) and are used for training/evaluation of circuit optimization and approximation.

### Training Data (error rate = 0.10)

Randomly generated circuits (e.g., 8-input / 2-output random benchmarks). The suffix `0.10` indicates the **allowed output error rate**.

```bash
wget https://huggingface.co/datasets/[ANONYMOUS]/circuit-transformer/resolve/main/8_inputs_2_outputs_random_deepsyn.zip
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/random_circuit_0.1_200k.zip
```

### Test Data

Small-scale benchmark set for quick functional testing.

```bash
wget https://huggingface.co/datasets/[ANONYMOUS]/circuit-transformer/resolve/main/test_data.zip
```

### Approximate Circuit Benchmarks (ALSRAC, error rate = {1%, 5%, 10%})

Approximate circuit benchmarks derived from IWLS, with target error rates \( \delta \in \{0.01, 0.05, 0.10\} \).

```bash
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/IWLS_FFWs_app_0.01.zip
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/IWLS_FFWs_app_0.05.zip
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/IWLS_FFWs_app_0.1.zip
```

### Pretrained / Finetuned Models

- **Finetuned model** (IWLS approximate circuits, error rate = 10%):

```bash
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/iwls_finetune_7_6_0.1
```

- **Example outputs** (AAG files generated by finetuned vs. base model on IWLS):

```bash
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/aag_after_finetune.zip
wget https://huggingface.co/datasets/[ANONYMOUS]/Approximate-Circuit-Transformer/resolve/main/aag_before_finetune.zip
```

- **Pretrained weights** for direct inference / evaluation:

```bash
wget https://huggingface.co/[ANONYMOUS]/circuit-transformer/tree/main
```

> The pretrained model weights are provided for **direct inference and evaluation**. Re-training from scratch is **not required** to reproduce the main results.

## Environment Setup

### Dependencies

```bash
pip install -r requirements.txt
```

### Tooling Notes (ABC / AIGER utilities)

This repo bundles the required binaries under `gtac/bin/` and resolves them automatically in `gtac/utils.py`.

- On Linux, ensure they are executable (e.g., `chmod +x gtac/bin/abc gtac/bin/aigtoaig`).
- On Windows, the bundled binaries should work without `chmod`.

### TensorFlow / CUDA Notes

- TensorFlow version requirement:

```bash
pip install tensorflow[and-cuda]==2.18.1
```

- If you encounter XLA-related CUDA path issues, set:

```bash
export XLA_FLAGS="--xla_gpu_cuda_data_dir=/path/to/your/cuda/nvvm/libdevice/"
```

### Optional: TensorFlow Models

This project optionally relies on `tensorflow/models` for certain components. If not required by your workflow, this step can be skipped.

```bash
git clone https://github.com/tensorflow/models.git
cd models
pip install .
```

## Usage

This repository intentionally provides a **single runnable example** for review-time evaluation: `scripts/utilities/run_example.py` (training + inference).

### Inference (pretrained weights)

Runs inference on two embedded AIGER examples and prints AND-count reduction + equivalence checks.

```bash
python scripts/utilities/run_example.py --load_hf --run_inference --run_mcts
```

### Pretraining (optional)

Trains on a local dataset directory and (optionally) runs inference afterwards.

```bash
python scripts/utilities/run_example.py ^
  --train ^
  --train_data_dir ./datasets/t/IWLS_FFWs ^
  --ckpt_save_path ./ckpt ^
  --epochs 10 ^
  --batch_size 16 ^
  --run_inference
```

## Reproducibility Notes

- The training data shuffle uses a fixed seed by default (configurable via the example script).
- Reported results may vary across GPU / CUDA / driver versions; please include your hardware details when reporting numbers.

## Acknowledgements

This project references the implementation of the [Circuit Transformer code](https://github.com/snowkylin/circuit-transformer) repository.

## License & Anonymity

- **License**: see `LICENSE`.
- **Anonymity Note**: All links and resources are anonymized for double-blind review. Any identifying information will be released upon acceptance.
